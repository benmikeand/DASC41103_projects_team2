{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eed958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn   # neural network module from PyTorch needed for building models\n",
    "import torchvision  # computer vision library built on top of PyTorch needed for datasets and models\n",
    "#from IPython.display import Image   # display images in Jupyter notebooks needed for visualizing images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a24e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a user defined function Dataset class\n",
    "# by using the \"duner\" or double-underscore to create methods we are creating special methods in Python that\n",
    "#     python recognizes to call them automaticaly in response to built-in operations, rather than invoking them explicityly ourselves\n",
    "class Joint_UDF_Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    # returns the corresponding sample to the given index\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffe637",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pathlib.Path('../data/images')\n",
    "\n",
    "# grab all file paths\n",
    "image_files = sorted([str(path) for path in images.glob('*_*.*')])\n",
    "\n",
    "# verify files\n",
    "print(f\"number of images: {len(image_files)} \\nfirst 10:\\n{image_files[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ef699",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "for i, file in enumerate(image_files[:6]):  # Only show first 6 images to fit 2x3 grid\n",
    "    img = Image.open(file) \n",
    "    print('Image shape: ', np.array(img).shape)\n",
    "    ax = fig.add_subplot(2, 3, i+1)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(os.path.basename(file), size=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull labels from file names and create label scheme\n",
    "labels = [1 if '_1' in os.path.basename(file) else 0 # 1 if uses official logo\n",
    "          for file in image_files]\n",
    "print(f\"labels: {labels}\")\n",
    "\n",
    "joint_UDF_dataset = Joint_UDF_Dataset(image_files, labels)\n",
    "\n",
    "# print first 10\n",
    "count = 1\n",
    "while count <= 10:\n",
    "    print(' x: ', joint_UDF_dataset[0], ' y: ', joint_UDF_dataset[1])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9346098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ImageDataset class for loading images with optional transformations\n",
    "# This class will be used with different transformation pipelines for exploration and training\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.file_list[index])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[index]\n",
    "        return img, label\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create an initial dataset with basic transformation (ToTensor + Resize) for exploration\n",
    "# This will be visualized and then replaced with enhanced transformations for training\n",
    "img_height, img_width = 200, 200\n",
    "\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "])\n",
    "\n",
    "# Initialize dataset for exploration and visualization\n",
    "image_dataset = ImageDataset(image_files, labels, basic_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d15db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 50 images from the dataset using pixels instead of opening each file\n",
    "num_images = 50\n",
    "num_cols = 10\n",
    "num_rows = 5\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "for i, example in enumerate(image_dataset):\n",
    "    if i >= num_images:\n",
    "        break\n",
    "    ax = fig.add_subplot(num_rows, num_cols, i+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Clamp the tensor values to the range [0, 1] before displaying\n",
    "    img_display = torch.clamp(example[0], 0, 1)\n",
    "\n",
    "    ax.imshow(img_display.numpy().transpose((1, 2, 0)))\n",
    "    ax.set_title(f'{example[1]}', size=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385a384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in to test and train sets\n",
    "image_path = images\n",
    "\n",
    "train_dataset_size = int(0.8 * len(image_dataset))\n",
    "test_dataset_size = len(image_dataset) - train_dataset_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(image_dataset, [train_dataset_size, test_dataset_size])\n",
    "\n",
    "print('Train set:', len(train_dataset))\n",
    "print('Test set:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORATION: Visualize the effects of individual transformation steps\n",
    "# This demonstrates what each transformation does to an image from the training split\n",
    "# Overall: shows original → random crop → horizontal flip → resize progression\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 12))\n",
    "\n",
    "for i, (img, attr) in enumerate(train_dataset):\n",
    "    # helper to display a tensor image (C,H,W) with matplotlib\n",
    "    def imshow_tensor(ax, tensor_img):\n",
    "        t = torch.clamp(tensor_img, 0, 1)  # ensure within [0,1]\n",
    "        npimg = t.permute(1, 2, 0).numpy()  # (C,H,W) -> (H,W,C)\n",
    "        ax.imshow(npimg)\n",
    "\n",
    "    # Column 1: Original image\n",
    "    ax = fig.add_subplot(3, 4, i*4+1)\n",
    "    imshow_tensor(ax, img)\n",
    "    if i == 0:\n",
    "        ax.set_title('Orig.', size=15)\n",
    "\n",
    "    # Column 2: After RandomCrop (resize first to ensure crop fits)\n",
    "    ax = fig.add_subplot(3, 4, i*4+2)\n",
    "    img_resized = transforms.functional.resize(img, size=(200, 200))\n",
    "    img_cropped = transforms.functional.crop(img_resized, top=0, left=0, height=178, width=178)\n",
    "    imshow_tensor(ax, img_cropped)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 1: Random crop (178×178)', size=15)\n",
    "\n",
    "    # Column 3: After RandomHorizontalFlip\n",
    "    ax = fig.add_subplot(3, 4, i*4+3)\n",
    "    img_flip = transforms.functional.hflip(img_cropped)\n",
    "    imshow_tensor(ax, img_flip)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 2: Random flip', size=15)\n",
    "\n",
    "    # Column 4: After final Resize to 500×500\n",
    "    ax = fig.add_subplot(3, 4, i*4+4)\n",
    "    img_final = transforms.functional.resize(img_flip, size=(500, 500))\n",
    "    imshow_tensor(ax, img_final)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 3: Resize (500×500)', size=15)\n",
    "\n",
    "    # Limit visualization to first 3 images for clarity\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation pipelines for training and evaluation\n",
    "# Training: applies random augmentations (crop, flip) to improve model generalization\n",
    "# Evaluation: uses deterministic transformations (center crop) for consistent results\n",
    "\n",
    "# IMPORTANT: Resize to larger size FIRST, then crop, to avoid extreme distortion\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize([256, 256]),        # Resize to larger size first (before crop)\n",
    "    transforms.RandomCrop([224, 224]),    # Now crop has plenty of pixels to work with\n",
    "    transforms.RandomHorizontalFlip(),    # Random horizontal flip\n",
    "    transforms.Resize([500, 500]),        # Final resize to target size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Evaluation transform: center crop instead of random, for consistency\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize([256, 256]),        # Resize to larger size first\n",
    "    transforms.CenterCrop([224, 224]),    # Center crop (deterministic)\n",
    "    transforms.Resize([500, 500]),        # Final resize to target size\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and evaluation DataLoaders with augmented transformations\n",
    "# Recreate train_dataset with enhanced transform_train for data augmentation during training\n",
    "\n",
    "train_dataset_augmented = ImageDataset(\n",
    "    file_list=[image_files[i] for i in train_dataset.indices],\n",
    "    labels=[labels[i] for i in train_dataset.indices],\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "# Create test dataset with evaluation transforms (no augmentation, deterministic)\n",
    "test_dataset_augmented = ImageDataset(\n",
    "    file_list=[image_files[i] for i in test_dataset.indices],\n",
    "    labels=[labels[i] for i in test_dataset.indices],\n",
    "    transform=transform_eval\n",
    ")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Create DataLoaders for training and evaluation with larger batch sizes\n",
    "# Larger batch_size reduces noise and overfitting risk\n",
    "train_data_loader = DataLoader(train_dataset_augmented, batch_size=8, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset_augmented, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add268e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the same images with different random augmentations across multiple epochs\n",
    "# This demonstrates how data augmentation creates variation in the same images\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Get the original unaugmented images from the train dataset (before augmentation)\n",
    "torch.manual_seed(1)\n",
    "original_images = []\n",
    "for idx in range(2):  # Get first 2 images\n",
    "    img_path = train_dataset_augmented.file_list[idx]\n",
    "    img = Image.open(img_path)\n",
    "    original_images.append(img)\n",
    "\n",
    "num_epochs = 5\n",
    "for j in range(num_epochs):\n",
    "    for img_idx, original_img in enumerate(original_images):\n",
    "        # Apply transform_train to the same original image multiple times\n",
    "        # Each time creates different augmentation due to RandomCrop and RandomHorizontalFlip\n",
    "        augmented_img = transform_train(original_img)\n",
    "        \n",
    "        ax = fig.add_subplot(2, 5, j + 1 if img_idx == 0 else j + 6)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f'Epoch {j}, Img {img_idx+1}', size=12)\n",
    "        ax.imshow(augmented_img.permute(1, 2, 0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce514c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 9 explain overall\n",
    "# overall, this box creates DataLoaders for the training, validation, and test datasets with a specified batch size\n",
    "batch_size = 5\n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd05325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 10 explain what this does overall and what it creates\n",
    "# overall, this box defines a convolutional neural network (CNN) model using PyTorch's nn.Sequential and creates layers with specific configurations\n",
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module('conv1', nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('pool1', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module('dropout1', nn.Dropout(p=0.5))\n",
    "\n",
    "model.add_module('conv2', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1))\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('pool2', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module('dropout2', nn.Dropout(p=0.5))\n",
    "\n",
    "model.add_module('conv3', nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1))\n",
    "model.add_module('relu3', nn.ReLU())\n",
    "model.add_module('pool3', nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "model.add_module('conv4', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1))\n",
    "model.add_module('relu4', nn.ReLU())\n",
    "\n",
    "# ensure fixed-size feature vector regardless of input spatial size\n",
    "model.add_module('pool4', nn.AdaptiveAvgPool2d((1, 1)))\n",
    "model.add_module('flatten', nn.Flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214af56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 11 explain overall and why we did this\n",
    "# overall, this box tests the CNN model by passing a dummy input tensor and checking the output shape\n",
    "\n",
    "x = torch.ones((4, 3, 64, 64))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 12\n",
    "# overall, this box continues defining the CNN model and tests it again with a dummy input tensor\n",
    "# what does this do. Can we exclude? Hint:  look at chapter\n",
    "# replace fixed-kernel avg pooling with adaptive pooling so the spatial output is always 1x1\n",
    "# this ensures the flattened feature vector has size 256 regardless of input image size\n",
    "model.add_module('pool4', nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "# what does this do\n",
    "# this line flattens the output from the convolutional layers to prepare it for fully connected layers\n",
    "model.add_module('flatten', nn.Flatten())\n",
    "\n",
    "# what does this do..does it provide expected size\n",
    "# now the flattened feature size will be 256 (channels) * 1 * 1 = 256\n",
    "x = torch.ones((4, 3, 64, 64))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3fc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 13\n",
    "# overall, this box adds the final layers to the CNN model for binary classification\n",
    "# NOTE: we do NOT add a Sigmoid here -- we will use BCEWithLogitsLoss which expects raw logits\n",
    "# this is numerically more stable than using a separate Sigmoid + BCELoss\n",
    "\n",
    "# add final fully-connected layer mapping flattened features to 1 logit\n",
    "# after AdaptiveAvgPool2d((1,1)) + Flatten the feature size should be 256\n",
    "model.add_module('fc', nn.Linear(256, 1))\n",
    "\n",
    "# do NOT add nn.Sigmoid() here when using BCEWithLogitsLoss\n",
    "# model.add_module('sigmoid', nn.Sigmoid())  # removed in favor of BCEWithLogitsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd778078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 14 explain overall....does it provide expected size?\n",
    "# overall, this box tests the complete CNN model by passing a dummy input tensor and checking the output shape\n",
    "x = torch.ones((4, 3, 64, 64))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 15 explain what this does\n",
    "# this line prints the architecture of the defined CNN\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba4c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 16\n",
    "# overall, this box sets up the device for computation and moves the model to that device\n",
    "# what does this do\n",
    "# this line sets the computation device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# what does this do\n",
    "# this line moves the model to the specified computation device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ddc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 17\n",
    "# overall, this box defines the loss function and optimizer for training the CNN model\n",
    "# Use BCEWithLogitsLoss so the model can output raw logits (more stable than separate sigmoid + BCELoss)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# define the Adam optimizer with weight decay (L2 regularization) to reduce overfitting\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dc1910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box 18\n",
    "# Training function - CLEAN REBUILD\n",
    "\n",
    "def train(model, num_epochs, train_dl, test_dl):\n",
    "    loss_hist_train = [0.0] * num_epochs\n",
    "    accuracy_hist_train = [0.0] * num_epochs\n",
    "    loss_hist_test = [0.0] * num_epochs\n",
    "    accuracy_hist_test = [0.0] * num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x_batch).squeeze(1)\n",
    "            loss = loss_fn(pred, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
    "            is_correct = ((torch.sigmoid(pred) >= 0.5).float() == y_batch).float()\n",
    "            accuracy_hist_train[epoch] += is_correct.sum().detach().cpu().item()\n",
    "\n",
    "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_dl:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                pred = model(x_batch).squeeze(1)\n",
    "                loss = loss_fn(pred, y_batch.float())\n",
    "                loss_hist_test[epoch] += loss.item() * y_batch.size(0)\n",
    "                is_correct = ((torch.sigmoid(pred) >= 0.5).float() == y_batch).float()\n",
    "                accuracy_hist_test[epoch] += is_correct.sum().detach().cpu().item()\n",
    "\n",
    "        loss_hist_test[epoch] /= len(test_dl.dataset)\n",
    "        accuracy_hist_test[epoch] /= len(test_dl.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {loss_hist_train[epoch]:.4f} | Train Acc: {accuracy_hist_train[epoch]:.4f} | Val Acc: {accuracy_hist_test[epoch]:.4f}')\n",
    "\n",
    "    return loss_hist_train, loss_hist_test, accuracy_hist_train, accuracy_hist_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f420521e",
   "metadata": {},
   "source": [
    "\n",
    "## Why Validation Accuracy Decreases & How to Fix It\n",
    "\n",
    "**Problem:** Val accuracy drops as epochs increase (e.g., 0.5 → 0.4) while train accuracy stays high = **overfitting**.\n",
    "\n",
    "**Root causes (for small datasets like yours):**\n",
    "1. **Tiny dataset (50 images)** — model memorizes training set instead of learning generalizable features\n",
    "2. **Batch size too small (batch_size=2)** — adds noise, increases overfitting risk\n",
    "3. **Model is too complex** — 4 conv layers can overfit on 40 training images\n",
    "4. **Not enough regularization** — dropout + weight decay help but need stronger measures\n",
    "\n",
    "**Solutions applied:**\n",
    "- ✅ Increased batch_size from 2 → 8 (reduces noise, stabilizes gradients)\n",
    "- ✅ Added weight_decay=0.0001 to optimizer (L2 regularization penalizes large weights)\n",
    "- ✅ You have dropout=0.5 in first 3 conv blocks (good!)\n",
    "\n",
    "**Optional improvements to try:**\n",
    "1. **Reduce learning rate** (slower, more careful updates):\n",
    "   - Change lr=0.001 → lr=0.0001\n",
    "2. **Increase dropout** in more layers:\n",
    "   - Add dropout after every ReLU (not just conv layers)\n",
    "3. **Early stopping** (stop training when val acc stops improving):\n",
    "   - Monitor val_acc; if no improvement for 3 epochs, stop\n",
    "4. **Simpler model** (fewer parameters):\n",
    "   - Reduce filters: 32→16, 64→32, etc.\n",
    "5. **More data augmentation** (make training harder):\n",
    "   - Add RandomRotation, ColorJitter, RandomAffine to transform_train\n",
    "6. **Ensemble approach** (train multiple models, average predictions)\n",
    "\n",
    "For now, re-run training with batch_size=8 and weight_decay. Val accuracy should stabilize better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35188b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_data_loader.dataset:\", len(train_data_loader.dataset))\n",
    "print(\"test_data_loader.dataset: \", len(test_data_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27aa5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting training with batch_size=8, weight_decay=0.0001, num_epochs=20\n",
      "================================================================================\n",
      "Epoch 1/20 | Train Loss: 0.3849 | Train Acc: 0.8500 | Val Acc: 0.6000\n",
      "Epoch 1/20 | Train Loss: 0.3849 | Train Acc: 0.8500 | Val Acc: 0.6000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training with batch_size=8, weight_decay=0.0001, num_epochs=20\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[43], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_dl, test_dl)\u001b[0m\n\u001b[1;32m     16\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(x_batch)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y_batch\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m loss_hist_train[epoch] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m y_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# box 19\n",
    "# Training execution cell - FRESH\n",
    "\n",
    "torch.manual_seed(1)\n",
    "num_epochs = 20\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Starting training with batch_size=8, weight_decay=0.0001, num_epochs=20\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "hist = train(model, num_epochs, train_data_loader, test_data_loader)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
